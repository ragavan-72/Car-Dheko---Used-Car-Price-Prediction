{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd484d8-5171-49b5-8283-596879e4baec",
   "metadata": {},
   "source": [
    "LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d8e4e4-0ffe-4779-a06e-a859969e8c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16048\\3403365003.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Train-----\n",
      "train_mse: 416128323682.7289\n",
      "train_r2: 0.6118384601804481\n",
      "-----Test-----\n",
      "test_mse: 567205982446.4929\n",
      "test_r2: 0.5813106721528956\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear'] #Car_age_calculating\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Separate features and target variable\n",
    "x = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "train_prediction = model.predict(x_train)\n",
    "test_prediction = model.predict(x_test)\n",
    "\n",
    "print('-----Train-----')\n",
    "train_mse = mean_squared_error(y_train, train_prediction)\n",
    "train_r2 = r2_score(y_train, train_prediction)\n",
    "print(f\"train_mse: {train_mse}\")\n",
    "print(f\"train_r2: {train_r2}\")\n",
    "\n",
    "print('-----Test-----')\n",
    "test_mse = mean_squared_error(y_test, test_prediction)\n",
    "test_r2 = r2_score(y_test, test_prediction)\n",
    "print(f\"test_mse: {test_mse}\")\n",
    "print(f\"test_r2: {test_r2}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d671a8bd-e5a9-4a24-86a8-31252203712d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16048\\1406854867.py:19: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for DecisionTree: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "Best parameters for RandomForest: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best parameters for GradientBoosting: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}\n",
      "\n",
      "Model: DecisionTree\n",
      "-----Train-----\n",
      "train_mae: 147830.1533556213\n",
      "train_mse: 121258845581.59732\n",
      "train_r2: 0.8919805085883908\n",
      "-----Test-----\n",
      "test_mae: 189913.90399999978\n",
      "test_mse: 241571121633.35797\n",
      "test_r2: 0.7961877696491189\n",
      "\n",
      "Model: RandomForest\n",
      "-----Train-----\n",
      "train_mae: 53371.16787144558\n",
      "train_mse: 18993580467.222473\n",
      "train_r2: 0.9830801877395886\n",
      "-----Test-----\n",
      "test_mae: 129735.07914036594\n",
      "test_mse: 103717951364.5865\n",
      "test_r2: 0.9124937333067316\n",
      "\n",
      "Model: GradientBoosting\n",
      "-----Train-----\n",
      "train_mae: 57745.91421653722\n",
      "train_mse: 6356767192.761627\n",
      "train_r2: 0.9943372810792426\n",
      "-----Test-----\n",
      "test_mae: 124833.78578816922\n",
      "test_mse: 73570567759.75194\n",
      "test_r2: 0.9379289155015245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "\n",
    "\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear'] #Car_age_calculating\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns #encoding\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "x = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 5]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid[name], cv=5, scoring='r2')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best models\n",
    "for name, model in best_models.items():\n",
    "    train_prediction = model.predict(x_train)\n",
    "    test_prediction = model.predict(x_test)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    \n",
    "    print('-----Train-----')\n",
    "    train_mae = mean_absolute_error(y_train, train_prediction)\n",
    "    train_mse = mean_squared_error(y_train, train_prediction)\n",
    "    train_r2 = r2_score(y_train, train_prediction)\n",
    "    print(f\"train_mae: {train_mae}\")\n",
    "    print(f\"train_mse: {train_mse}\")\n",
    "    print(f\"train_r2: {train_r2}\")\n",
    "    \n",
    "    print('-----Test-----')\n",
    "    test_mae = mean_absolute_error(y_test, test_prediction)\n",
    "    test_mse = mean_squared_error(y_test, test_prediction)\n",
    "    test_r2 = r2_score(y_test, test_prediction)\n",
    "    print(f\"test_mae: {test_mae}\")\n",
    "    print(f\"test_mse: {test_mse}\")\n",
    "    print(f\"test_r2: {test_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73551897-8e8f-435e-b5c6-750f284905cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16048\\1266328538.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear']  # Car_age_calculating\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns  # encoding\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "x = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.2, max_depth=5, n_estimators=200)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "train_pred = gb.predict(x_train)\n",
    "test_pred = gb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d918d7-a7f7-42f8-bff9-3776daeb8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a .pkl file\n",
    "with open('CDmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(gb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939fb3cc-2d47-4356-b738-900e43c37ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 25648773981.119724\n",
      "Test MSE: 24749895672.33688\n",
      "Train R^2: 0.9772853584479851\n",
      "Test R^2: 0.9780470414264774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16048\\3365172184.py:18: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear']  # Car_age_calculating\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns  # encoding\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "x = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# Load the model from the .pkl file\n",
    "with open('CDmodel.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "train_pred = loaded_model.predict(x_train)\n",
    "test_pred = loaded_model.predict(x_test)\n",
    "\n",
    "print(f\"Train MSE: {mean_squared_error(y_train, train_pred)}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, test_pred)}\")\n",
    "print(f\"Train R^2: {r2_score(y_train, train_pred)}\")\n",
    "print(f\"Test R^2: {r2_score(y_test, test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f8ee1-6501-40f5-9deb-2c16fa154bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed41546-ebbf-41fa-9e72-5d879199e0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64e233-e1e5-40e9-a5cf-187fa233d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "\n",
    "# Calculate the age of the car\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear']\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Fill NaN values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Splitting the data\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Define a pipeline with scaling and model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', ElasticNet())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Bayesian Optimization\n",
    "param_grid = {\n",
    "    'model__alpha': (0.1, 100.0, 'log-uniform'),\n",
    "    'model__l1_ratio': (0.0, 1.0)\n",
    "}\n",
    "\n",
    "# Implementing k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Bayesian Optimization with cross-validation\n",
    "bayes_search = BayesSearchCV(pipeline, param_grid, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1, n_iter=50, random_state=42)\n",
    "bayes_search.fit(X_train_poly, y_train)\n",
    "\n",
    "# Best model from Bayesian Optimization\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train_poly)\n",
    "y_test_pred = best_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluation\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"-----Train-----\")\n",
    "print(f\"train_mae: {train_mae}\")\n",
    "print(f\"train_mse: {train_mse}\")\n",
    "print(f\"train_r2: {train_r2}\")\n",
    "\n",
    "print(\"-----Test-----\")\n",
    "print(f\"test_mae: {test_mae}\")\n",
    "print(f\"test_mse: {test_mse}\")\n",
    "print(f\"test_r2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f7b619-85bd-4ef4-9f86-7c45f7720a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2cca85-1386-4ccd-9f1b-6b71f16b993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return mae, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20f4250-84ee-4b46-b1f2-3ed3fedb7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MAE: 396614.11778851406, MSE: 567204007529.4952, R2: 0.581312129959576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ragavan lingam\\AppData\\Local\\Temp\\ipykernel_28440\\1238569325.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Cardheko_final.csv')\n",
    "\n",
    "# Calculate the age of the car\n",
    "df['Age_of_the_Car'] = 2024 - df['modelYear']\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Fill NaN values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "X = df.drop('price', axis=1)  \n",
    "y = df['price']  \n",
    "# Split the dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "mae_lr, mse_lr, r2_lr = evaluate_model(lr, X_test, y_test)\n",
    "print(f\"Linear Regression - MAE: {mae_lr}, MSE: {mse_lr}, R2: {r2_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4b77fd-9c24-4070-b334-8950bdb8b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - MAE: 185259.1253427112, MSE: 250263789890.8981, R2: 0.8152650338384368\n",
      "Best Decision Tree Params: {'max_depth': 20, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "# 2. Decision Tree with Grid Search\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20,30,40,50],\n",
    "    'min_samples_split': [2, 5, 10,30,50]\n",
    "}\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "mae_dt, mse_dt, r2_dt = evaluate_model(best_dt, X_test, y_test)\n",
    "print(f\"Decision Tree - MAE: {mae_dt}, MSE: {mse_dt}, R2: {r2_dt}\")\n",
    "print(f\"Best Decision Tree Params: {grid_search_dt.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853c9f36-d079-4832-85f6-37bf6ed2a447",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m gb \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m grid_search_gb \u001b[38;5;241m=\u001b[39m GridSearchCV(gb, param_grid_gb, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m grid_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     10\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m grid_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     11\u001b[0m mae_gb, mse_gb, r2_gb \u001b[38;5;241m=\u001b[39m evaluate_model(best_gb, X_test, y_test)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:784\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    783\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    785\u001b[0m     X_train,\n\u001b[0;32m    786\u001b[0m     y_train,\n\u001b[0;32m    787\u001b[0m     raw_predictions,\n\u001b[0;32m    788\u001b[0m     sample_weight_train,\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    790\u001b[0m     X_val,\n\u001b[0;32m    791\u001b[0m     y_val,\n\u001b[0;32m    792\u001b[0m     sample_weight_val,\n\u001b[0;32m    793\u001b[0m     begin_at_stage,\n\u001b[0;32m    794\u001b[0m     monitor,\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    797\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:880\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    873\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    874\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    875\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    876\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    877\u001b[0m         )\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    881\u001b[0m     i,\n\u001b[0;32m    882\u001b[0m     X,\n\u001b[0;32m    883\u001b[0m     y,\n\u001b[0;32m    884\u001b[0m     raw_predictions,\n\u001b[0;32m    885\u001b[0m     sample_weight,\n\u001b[0;32m    886\u001b[0m     sample_mask,\n\u001b[0;32m    887\u001b[0m     random_state,\n\u001b[0;32m    888\u001b[0m     X_csc\u001b[38;5;241m=\u001b[39mX_csc,\n\u001b[0;32m    889\u001b[0m     X_csr\u001b[38;5;241m=\u001b[39mX_csr,\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:490\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    487\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    489\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 490\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    491\u001b[0m     X, neg_g_view[:, k], sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    495\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1378\u001b[0m         X,\n\u001b[0;32m   1379\u001b[0m         y,\n\u001b[0;32m   1380\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1381\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1382\u001b[0m     )\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Gradient Boosting with Grid Search\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [300,400,800,1000, 1500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3,7,10]\n",
    "}\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "mae_gb, mse_gb, r2_gb = evaluate_model(best_gb, X_test, y_test)\n",
    "print(f\"Gradient Boosting - MAE: {mae_gb}, MSE: {mse_gb}, R2: {r2_gb}\")\n",
    "print(f\"Best Gradient Boosting Params: {grid_search_gb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba74c98-7caa-4659-9290-0b31f2daaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Random Forest with Grid Search\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [400,800,1000, 1500],\n",
    "    'max_depth': [None, 10, 20,40,50],\n",
    "    'min_samples_split': [2, 5, 10,30,50]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "mae_rf, mse_rf, r2_rf = evaluate_model(best_rf, X_test, y_test)\n",
    "print(f\"Random Forest - MAE: {mae_rf}, MSE: {mse_rf}, R2: {r2_rf}\")\n",
    "print(f\"Best Random Forest Params: {grid_search_rf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab26259-e13f-41fc-90df-c9f06baf97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. K-Nearest Neighbors (KNN) with Grid Search\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan, 2 for Euclidean\n",
    "}\n",
    "knn = KNeighborsRegressor()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "mae_knn, mse_knn, r2_knn = evaluate_model(best_knn, X_test, y_test)\n",
    "print(f\"KNN - MAE: {mae_knn}, MSE: {mse_knn}, R2: {r2_knn}\")\n",
    "print(f\"Best KNN Params: {grid_search_knn.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc47a56-51c6-45c7-8a59-58060c99812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results for each model\n",
    "results = {\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'KNN'],\n",
    "    'MAE': [mae_lr, mae_dt, mae_rf, mae_gb, mae_knn],\n",
    "    'MSE': [mse_lr, mse_dt, mse_rf, mse_gb, mse_knn],\n",
    "    'R2': [r2_lr, r2_dt, r2_rf, r2_gb, r2_knn]\n",
    "}\n",
    "# Create DataFrame for comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
